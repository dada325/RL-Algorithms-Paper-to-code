{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPBP91pE5FYmDeAH6Jxn0dG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","# Soft Actor-Critic\n","using state value function\n","paper: https://arxiv.org/pdf/1801.01290.pdf\n"],"metadata":{"id":"JWSJwAFi53kZ"}},{"cell_type":"markdown","source":["### Import the necessry libraries"],"metadata":{"id":"ZiDngZVf59Gv"}},{"cell_type":"code","source":["import jax\n","import jax.numpy as jnp\n","\n","from flax import linen as nn\n","from flax.training import train_state\n","from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n","import optax\n","\n","import gym\n","import numpy as np"],"metadata":{"id":"dcf4A3Jm6FQc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define your networks:"],"metadata":{"id":"PZfdgSf16bHI"}},{"cell_type":"code","source":["class ValueNetwork(nn.Module):\n","    hidden_dim: int\n","\n","    def setup(self):\n","        self.dense1 = nn.Dense(self.hidden_dim)\n","        self.dense2 = nn.Dense(self.hidden_dim)\n","        self.dense3 = nn.Dense(1)\n","\n","    def __call__(self, x):\n","        x = nn.relu(self.dense1(x))\n","        x = nn.relu(self.dense2(x))\n","        return self.dense3(x)\n","\n","#TO-DO: Define SoftQNetwork and PolicyNetwork similarly\n","\n","class SooftQNetwork():\n","\n","\n","\n","class PolicyNetwork():\n","\n","\n"],"metadata":{"id":"5Cvq7n376kU0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define your loss functions and update step:"],"metadata":{"id":"yM-BPdTz6xAh"}},{"cell_type":"code","source":["def value_loss_fn(params, target_value, predicted_value):\n","    return jnp.mean((predicted_value - target_value) ** 2)\n","\n","# TO-DO: Define soft_q_loss_fn and policy_loss_fn\n","\n","def soft_q_loss_fn(params):\n",""],"metadata":{"id":"h8Av_j3E6reg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@jax.jit\n","def train_step(state, batch):\n","    def loss_fn(params):\n","        # TO-DO: Compute losses here using the loss functions defined above\n","        return loss\n","\n","    grad_fn = jax.value_and_grad(loss_fn)\n","    loss, grad = grad_fn(state.params)\n","    return state.apply_gradients(grads=grad), loss"],"metadata":{"id":"0i34RMIa67zR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Initialize your networks and optimizers"],"metadata":{"id":"OM7AVa8E77HH"}},{"cell_type":"code","source":["value_net = ValueNetwork(hidden_dim=512)\n","params = value_net.init(jax.random.PRNGKey(0), jnp.ones([1, state_dim]))\n","\n","optimizer = optax.adam(3e-4)\n","state = train_state.TrainState.create(apply_fn=value_net.apply, params=params, tx=optimizer)\n"],"metadata":{"id":"evT_yM5o7yI6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define your training loop:"],"metadata":{"id":"_Ax4r0TG8JfH"}},{"cell_type":"code","source":["for eps in range(max_episodes):\n","    state = env.reset()\n","    episode_reward = 0\n","    for step in range(max_steps):\n","        # TO-DO: Collect experience and update networks using train_step"],"metadata":{"id":"vTWL4ffw8KgO"},"execution_count":null,"outputs":[]}]}